version: '3.8'

services:
  ai-core:
    image: ${DOCKER_USERNAME}/askfmi-ai-core:latest
    container_name: askfmi_ai_core_prod
    runtime: nvidia
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HUGGINGFACE_TOKEN=${HUGGINGFACE_TOKEN}
      # If model path is needed, we can mount it or download it
    volumes:
      - ai_data:/app/data
      # Map model cache to host to avoid re-downloading on every deploy
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
    shm_size: '8gb'
    ulimits:
      memlock: -1
      stack: 67108864

volumes:
  ai_data:
